# ml-final-project

 The project has two goals for me. The first – summarize all knowledge I have got from the course, the second – trying to use machine learning to forecast (in our case classification) POI using row data. Started exploring data I found that out data contain 146 items with 21 features in each. The next step was – data visualization. I started with features_list = ['poi','salary']. I noticed 1 point looked like an outlier. More detailed exploration data set, I found it's 'Total' key – so obvious it's  wrong data so I removed it. Also I found that after call featureFormat function, number of features less than number point in data set. I found that 'LOCKHART EUGENE E' don't have any set feature – so I removed it point also. So after removing outliers(in my opinion) – we have 144 points, 18 of them labeled as poi. The next exploration step was features completeness – all features contain missed data, so I thought that features with less missed data were good candidates for start point.
	For start point I chosen ['total_stock_value', 'total_payments'], also  there were only (['CHAN RONNIE', 'POWERS WILLIAM']) names for which we doesn't have data in our selected features. So I started with features_list = ['poi','salary', 'total_stock_value', 'total_payments']. For the first iteration(and the rest) I used the next algorithms: GaussianNB, LinearSVC, DecisionTreeClassifier, RandomForestClassifier. I could get good the accuracy but precision  and   recall were terrible. Then I realized that some algorithm need rescaling data and I used MinMaxScaler.  For example for the started features_list – DecisionTreeClassifier feature importance was  - [ 0.5799447   0.19560564  0.22444966]
	My final features list  was ['poi','salary', 'total_stock_value', 'deferred_income'], and I ended with GaussianNB. It was a bit surprise to me.
As an additional experiment I decided to try creating new features using PCA  and select 5   component, the new features I used with  DecisionTreeClassifier(min_samples_split=15). I got the next results - Accuracy: 0.83227	Precision: 0.19789	Recall: 0.08450	F1: 0.11843 F2: 0.09544. 
	Tuning algorithm parameters are important part for good model performance.  Parameters help us to generalize model and avoid overfitting and underfitting. 
For example for DecisionTreeClassifier I tryied to tune algorithm using next parameters: criterion, min_samples_spli,  max_features.

	Validation is checking algorithm performance. We can make mistake by using all our data as training data(don't splitting data on test and train sets) and test algorithm performance on the some train data.   Also we can  split data in test and train sets but test performance on the train data again.
So we can't correct estimate algorithm performance. For analyzing of algorithm performance I used as first check point accuracy of the model and the next step precision and recall parameters.
	To evaluate algorithm we use 2 metrics precision and recall. Precision tells us ratio of many times algorithm correct  predict true positives to sum true positives + true negatives.  Recall is probability that algorithm makes correct prediction. For my final algorithm - recall: 0.34800 means than only 34 percent algorithm make correct  prediction. 
Precision: 0.51709 means  than only in 51 percent when algorithm makes prediction  is was correct prediction.
